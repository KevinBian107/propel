# CLAUDE.md

This project uses the [Propel](https://github.com/KevinBian107/propel) research workflow.
Skills, agents, and commands are in `.claude/` (installed via `propel init`).

## Research Context

<!-- What is this project? What problem does it solve? What domain is it in? -->


## Research Question

<!-- What are you testing or building? Be specific — not "implement X" but "test whether X improves Y under condition Z." -->


## Hypothesis

<!-- What do you expect to happen and why? This is what auditors verify against. -->


## Method

<!-- Which paper(s), which equations, which specific algorithmic choices? Link to papers, cite section numbers. Claude can't infer these from context. -->


## Domain-Specific Pitfalls

<!-- The things that silently fail in your domain. Examples:
- "Broadcasting in JAX will silently produce wrong shapes without error"
- "Loss decreasing doesn't mean the model is learning — check reconstruction quality"
- "Our config system uses nested dicts — changing a key name silently falls back to defaults"
-->


## Project Conventions

<!-- How is the code organized? What patterns must be followed? What should NOT be changed? -->


## Known Constraints

<!-- Hardware limits, dependency versions, compatibility requirements, performance targets -->


## What "Correct" Means Here

<!-- How do you verify this project works? Not just "tests pass" — what domain-specific checks matter? Examples:
- "Output must match equation 3 from [paper] within 1e-6"
- "Existing configs in configs/ must produce identical results before and after changes"
- "Latency must stay under 10ms per inference step"
-->

